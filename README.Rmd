---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Salvage @datasci_blogs's Tweets

## データ取得

```{r load-library}
library(rtweet)
library(dplyr, warn.conflicts = FALSE)
library(purrr)
library(readr)
library(tidyr)
```

```{r get-data, cache=file.exists("urls.txt")}
# 3200 is the limit of the API (c.f. https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline)
tw <- get_timeline("datasci_blogs", n = 3200)
```

## 記事URLを取り出す

### 記事URLがゼロ個のもの

URLが含まれないツイートは3200件中16件。無視。RSSの設定が間違ってた場合とか？

```{r wrong-tweets, cache=file.exists("urls.txt")}
tw %>%
  filter(is.na(urls_expanded_url)) %>%
  pull(text)
```

### 記事URLが1つのもの

そのまま使う

```{r split-tweet, cache=file.exists("urls.txt")}
tw_list <- tw %>%
  select(status_id, created_at, urls_expanded_url, urls_t.co, text) %>%
  filter(!is.na(urls_expanded_url)) %>%
  split(map_int(.$urls_expanded_url, length) > 1)

urls_single <- map_chr(tw_list$`FALSE`$urls_expanded_url, 1)
```

### 記事URLが2つ以上のもの

`text`と照らし合わせて使う。

```{r urls_double, cache=file.exists("urls.txt")}
urls_double <- tw_list$`TRUE` %>%
  # メインのURLを取り出す
  mutate(urls_main = coalesce(
    stringr::str_match(text, "^【.*】 (https://t.co/[[:alnum:]]+) .*")[,2],
    stringr::str_match(text, "^【.*】 .* (https://t.co/[[:alnum:]]+)")[,2],
    stringr::str_match(text, stringr::regex("^【.*(https://t.co/[[:alnum:]]+)$", dotall = TRUE))[,2]
  )) %>%
  # それに対応するexpanded URLを取り出す
  pmap_chr(function(urls_main, urls_expanded_url, urls_t.co, ...) {
    unique(urls_expanded_url[urls_main == urls_t.co])
  })
```

### 結合して保存

```{r combine, cache=file.exists("urls.txt")}
urls <- c(urls_single, urls_double)
write_lines(urls, "urls.txt")
```

## 本当のURLを取得

ここはシェル芸でやった

```sh
for URL in $(cat urls.txt); do
  curl -s -L -I $URL | \
  awk -v"OFS=," -v"short=$URL" '$1 == "Location:" {real=$2}; END{print short,real}' | \
  tail -1
done | tee real_urls.txt
```

参考までに、たぶんRでやるならこう。

```{r curl, eval=FALSE}
library(curl)

get_location <- function(url, handle) {
  res <- curl_fetch_memory(url, handle = handle)
  parse_headers_list(res$headers)$location
}

h <- new_handle(followlocation = FALSE,
                customrequest = "HEAD",
                nobody = TRUE)

# WARNING: this takes several tens of minutes
r <- purrr::map(ifttt_urls, purrr::safely(get_location), handle = h)

# confirm there are no errors
purrr::keep(r, ~ !is.null(.$error))

ifttt_urls_table <- purrr::map_chr(r, "result")
```

## スクレイピング行為に及ぶ前にURLをもうちょっと眺める

```{r load_data}
urls <- read_csv("real_urls.txt", col_names = c("short", "real"))

urls_df <- tibble(urls = coalesce(urls$real, urls$short),
                  base_urls = stringr::str_extract(urls, "^https?://[^/]+/?"))

urls_df %>%
  count(base_urls, sort = TRUE) %>%
  filter(n > 1) %>%
  knitr::kable()
```

特別対応が必要そうなのは、

* http://d.hatena.ne.jp/
* https://qiita.com/
* http://rpubs.com/

くらい。github.comになってるやつは明らかにミスってるけど、まあ2件だけなので無視。

### そのままスクレイピングできるものはスクレイピング

```{r scrape, cache=file.exists("rss.csv")}
urls_scrape <- unique(urls_df$base_urls) %>%
  discard(. %in% c("http://d.hatena.ne.jp/", "https://qiita.com/", "http://rpubs.com/"))

library(rvest)

get_feeds <- function(url) {
  read_html(url) %>% 
    html_nodes(xpath = "//head/*[@rel='alternate']") %>%
    html_attrs() %>%
    map_dfr(as.list)
}

l <- map(urls_scrape, safely(get_feeds))

# エラーがあるかチェック
purrr::keep(l, ~ !is.null(.$error))
```

```{r scrape2, cache=file.exists("rss.csv")}
df_rss <- l %>%
  set_names(urls_scrape) %>%
  map_dfr("result", .id = "website")

write_csv(df_rss, path = "rss.csv")
```

#### うまくとれていないもの

個別対応が必要そう

```{r miss}
urls_scrape %>%
  discard(~ . %in% unique(df_rss$website))
```

### d.hatena.ne.jp

RSS 1.0と2.0があるけど片方でいいので2.0をとってくる。

```{r hatena}
library(stringr)

rss_hatena <- urls_df %>%
  filter(base_urls == "http://d.hatena.ne.jp/") %>%
  mutate(user = str_extract(.$urls, "(?<=^http://d.hatena.ne.jp/)[^/]+")) %>%
  transmute(
    website = str_c("http://d.hatena.ne.jp/", user),
    rel     = "alternate",
    type    = "application/rss+xml",
    title   = "RSS 2.0",
    href    = str_c(website, "/rss2")
  )
```

### Qiita

```{r qiita}
rss_qiita <- urls_df %>%
  filter(base_urls == "https://qiita.com/") %>%
  mutate(user = str_extract(.$urls, "(?<=^https://qiita.com/)[^/]+")) %>%
  transmute(
    website = str_c("https://qiita.com/", user),
    rel     = "alternate",
    type    = "application/atom+xml",
    title   = "Atom Feed",
    href    = str_c(website, "/feed")
  )
```

### Rpubs

RSSどこ...？

### うまくとれていないもの

わかったやつだけ。

```{r atatakami}
rss_manual <- tribble(
  ~website, ~href,
  "https://blogs.technet.microsoft.com/machinelearning/", "https://blogs.technet.microsoft.com/machinelearning/feed/",
  "https://blog.rstudio.com/", "https://blog.rstudio.com/index.xml",
  "http://appliedpredictivemodeling.com/blog/", "http://appliedpredictivemodeling.com/blog?format=RSS",
  "http://soqdoq.com/symposion/", "http://soqdoq.com/symposion/feed/",
  "http://www.buildingwidgets.com/blog", "http://www.buildingwidgets.com/blog?format=RSS",
  "https://elix-tech.github.io/", "https://elix-tech.github.io/feed.xml",
  "https://wiseodd.github.io/", "https://wiseodd.github.io/feed.xml",
  "http://threeprogramming.lolipop.jp/blog/", "http://threeprogramming.lolipop.jp/blog/?feed=rss2",
  "https://mosko.tokyo/post/", "https://mosko.tokyo/index.xml",
  "http://blog.gepuro.net/", "http://blog.gepuro.net/recent.atom",
  "http://dirk.eddelbuettel.com/blog/", "http://dirk.eddelbuettel.com/blog/index.rss",
  "http://blog.kz-md.net/", "http://blog.kz-md.net/?feed=rss2",
  "https://suryu.me/blog/", "https://suryu.me/blog/index.xml",
  "http://dustintran.com/blog/", "http://dustintran.com/blog/feed.xml",
  "http://austinrochford.com/", "http://austinrochford.com/rss.xml"
)
```

## 結合

```{r all-rss}
rss_all <- bind_rows(df_rss, rss_hatena, rss_qiita, rss_manual) %>%
  transmute(website,
            title,
            href = if_else(startsWith(href, "http"),
                           href,
                           str_c(website, str_sub(href, start = 2))))

write_csv(rss_all, "rss_all.csv")
```


## 結果

```{r result}
rss_all %>%
  knitr::kable(format = "markdown")
```
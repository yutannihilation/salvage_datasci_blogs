---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Salvage @datasci_blogs's Tweets

## データ取得

```{r load-library}
library(rtweet)
library(dplyr, warn.conflicts = FALSE)
library(purrr)
library(readr)
library(tidyr)
```

```{r get-data, cache=file.exists("urls.txt")}
# 3200 is the limit of the API (c.f. https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline)
tw <- get_timeline("datasci_blogs", n = 3200)
```

## 記事URLを取り出す

### 記事URLがゼロ個のもの

URLが含まれないツイートは3200件中16件。無視。RSSの設定が間違ってた場合とか？

```{r wrong-tweets, cache=file.exists("urls.txt")}
tw %>%
  filter(is.na(urls_expanded_url)) %>%
  pull(text)
```

### 記事URLが1つのもの

そのまま使う

```{r split-tweet, cache=file.exists("urls.txt")}
tw_list <- tw %>%
  select(status_id, created_at, urls_expanded_url, urls_t.co, text) %>%
  filter(!is.na(urls_expanded_url)) %>%
  split(map_int(.$urls_expanded_url, length) > 1)

urls_single <- map_chr(tw_list$`FALSE`$urls_expanded_url, 1)
```

### 記事URLが2つ以上のもの

`text`と照らし合わせて使う。

```{r urls_double, cache=file.exists("urls.txt")}
urls_double <- tw_list$`TRUE` %>%
  # メインのURLを取り出す
  mutate(urls_main = coalesce(
    stringr::str_match(text, "^【.*】 (https://t.co/[[:alnum:]]+) .*")[,2],
    stringr::str_match(text, "^【.*】 .* (https://t.co/[[:alnum:]]+)")[,2],
    stringr::str_match(text, stringr::regex("^【.*(https://t.co/[[:alnum:]]+)$", dotall = TRUE))[,2]
  )) %>%
  # それに対応するexpanded URLを取り出す
  pmap_chr(function(urls_main, urls_expanded_url, urls_t.co, ...) {
    unique(urls_expanded_url[urls_main == urls_t.co])
  })
```

### 結合して保存

```{r combine, cache=file.exists("urls.txt")}
urls <- c(urls_single, urls_double)
write_lines(urls, "urls.txt")
```

## 本当のURLを取得

ここはシェル芸でやった

```sh
for URL in $(cat urls.txt); do
  curl -s -L -I $URL | \
  awk -v"OFS=," -v"short=$URL" '$1 == "Location:" {real=$2}; END{print short,real}' | \
  tail -1
done | tee real_urls.txt
```

参考までに、たぶんRでやるならこう。

```{r curl, eval=FALSE}
library(curl)

get_location <- function(url, handle) {
  res <- curl_fetch_memory(url, handle = handle)
  parse_headers_list(res$headers)$location
}

h <- new_handle(followlocation = FALSE,
                customrequest = "HEAD",
                nobody = TRUE)

# WARNING: this takes several tens of minutes
r <- purrr::map(ifttt_urls, purrr::safely(get_location), handle = h)

# confirm there are no errors
purrr::keep(r, ~ !is.null(.$error))

ifttt_urls_table <- purrr::map_chr(r, "result")
```

## スクレイピング行為に及ぶ前にURLをもうちょっと眺める

```{r load_data}
urls <- read_csv("real_urls.txt", col_names = c("short", "real"))

urls_df <- tibble(urls = coalesce(urls$real, urls$short),
                  base_urls = stringr::str_extract(urls, "^https?://[^/]+/?"))

urls_df %>%
  count(base_urls, sort = TRUE) %>%
  filter(n > 1) %>%
  knitr::kable()
```

特別対応が必要そうなのは、

* http://d.hatena.ne.jp/
* https://qiita.com/
* http://rpubs.com/

くらい。github.comになってるやつは明らかにミスってるけど、まあ2件だけなので無視。